{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Your Data From Yelp!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make sure you are on track to completing the project, you will complete this workbook first. Below are steps that you need to take in order to make sure you have your data from yelp and are ready to analyze it. Your cohort lead will review this workbook with you the Wednesday before your project is due.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Understanding your data and question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be pulling data from the Yelp API to complete your analysis. The API, however, provides you with a lot of information that will not be pertinent to your analysis. You will pull data from the API and parse through it to keep only the data that you will need. In order to help you identify that information,look at the API documentation and understand what data the API will provide you. \n",
    "\n",
    "Identify which data fields you will want to keep for your analysis. \n",
    "\n",
    "https://www.yelp.com/developers/documentation/v3/get_started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Create ETL pipeline for the business data from the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know what data you need from the API, you want to write code that will execute an API call, parse those results and then insert the results into the DB.  \n",
    "\n",
    "It is helpful to break this up into three different functions (*API call, parse results, and insert into DB*) and then you can write a function/script that pull the other three functions together. \n",
    "\n",
    "Let's first do this for the Business endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:23.094040Z",
     "start_time": "2021-05-13T18:49:21.556038Z"
    },
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2024-03-14T22:33:01.932072Z",
     "iopub.status.busy": "2024-03-14T22:33:01.932072Z",
     "iopub.status.idle": "2024-03-14T22:33:03.346210Z",
     "shell.execute_reply": "2024-03-14T22:33:03.346210Z",
     "shell.execute_reply.started": "2024-03-14T22:33:01.932072Z"
    }
   },
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import cachetools.func\n",
    "import concurrent.futures\n",
    "import csv\n",
    "from functools import wraps, lru_cache\n",
    "import logging\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import requests\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T22:33:03.348959Z",
     "iopub.status.busy": "2024-03-14T22:33:03.348183Z",
     "iopub.status.idle": "2024-03-14T22:33:03.362426Z",
     "shell.execute_reply": "2024-03-14T22:33:03.362426Z",
     "shell.execute_reply.started": "2024-03-14T22:33:03.348959Z"
    }
   },
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:23.094040Z",
     "start_time": "2021-05-13T18:49:21.556038Z"
    },
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2024-03-14T22:33:03.362426Z",
     "iopub.status.busy": "2024-03-14T22:33:03.362426Z",
     "iopub.status.idle": "2024-03-14T22:33:03.378683Z",
     "shell.execute_reply": "2024-03-14T22:33:03.378069Z",
     "shell.execute_reply.started": "2024-03-14T22:33:03.362426Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('.secrets.json') as f:\n",
    "    keys = json.load(f)\n",
    "\n",
    "client_id = keys['Client_ID']\n",
    "yelp_key = keys['api_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT'ed Class and Functions for API Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T22:34:47.650072Z",
     "iopub.status.busy": "2024-03-14T22:34:47.650072Z",
     "iopub.status.idle": "2024-03-14T22:34:47.668471Z",
     "shell.execute_reply": "2024-03-14T22:34:47.668471Z",
     "shell.execute_reply.started": "2024-03-14T22:34:47.650072Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import logging\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from pandas import DataFrame\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "async def fetch_with_retry(session, url, headers, params, retries=3, backoff_factor=0.5):\n",
    "    \"\"\"Attempt to fetch data with retries and exponential backoff.\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            async with session.get(url, headers=headers, params=params) as response:\n",
    "                response.raise_for_status()\n",
    "                return await response.json()\n",
    "        except aiohttp.ClientResponseError as e:\n",
    "            if e.status == 503:\n",
    "                logging.warning(f\"Attempt {attempt + 1} failed with status 503. Retrying after {backoff_factor * (2 ** attempt)} seconds...\")\n",
    "                await asyncio.sleep(backoff_factor * (2 ** attempt))\n",
    "            else:\n",
    "                raise e\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                logging.error(f\"Failed after {retries} attempts.\")\n",
    "                raise e\n",
    "    logging.error(\"Max retries exceeded with 503 error.\")\n",
    "    return {}\n",
    "\n",
    "class YelpDataFrame(DataFrame):\n",
    "    _metadata = ['yelp_key', 'api_base_url', 'default_endpoint', 'api_url', '_headers', 'MAX_RESULTS', 'BATCH_SIZE']\n",
    "\n",
    "    def __init__(self, yelp_key, api_base_url='https://api.yelp.com/v3/', default_endpoint='businesses/search', max_results=1000, batch_size=50, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.yelp_key = yelp_key\n",
    "        self.api_base_url = api_base_url\n",
    "        self.default_endpoint = default_endpoint\n",
    "        self.api_url = f\"{self.api_base_url}{self.default_endpoint}\"\n",
    "        self._headers = {'Authorization': f'Bearer {self.yelp_key}'}\n",
    "        self.MAX_RESULTS = max_results\n",
    "        self.BATCH_SIZE = batch_size\n",
    "\n",
    "    @property\n",
    "    def headers(self):\n",
    "        return self._headers\n",
    "\n",
    "    @property\n",
    "    def _constructor(self):\n",
    "        return YelpDataFrame\n",
    "\n",
    "    async def _fetch_total_results_async(self, term, location):\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            try:\n",
    "                response = await fetch_with_retry(session, self.api_url, self.headers, {'term': term, 'location': location, 'limit': 1})\n",
    "                if response and 'total' in response:\n",
    "                    return max(0, response.get('total', 0))\n",
    "                else:\n",
    "                    logging.error(\"No 'total' field in the response.\")\n",
    "                    return 0\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to fetch total results: {e}\")\n",
    "                return 0\n",
    "\n",
    "    def _fetch_total_results_sync(self, term, location):\n",
    "        try:\n",
    "            params = {'term': term, 'location': location, 'limit': 1}\n",
    "            response = requests.get(self.api_url, headers=self.headers, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            if data and 'total' in data:\n",
    "                return max(0, data.get('total', 0))\n",
    "            else:\n",
    "                logging.error(\"No 'total' field in the response.\")\n",
    "                return 0\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to fetch total results: {e}\")\n",
    "            return 0\n",
    "\n",
    "    async def _fetch_data_async(self, term, location, total):\n",
    "        if total > 0:\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                tasks = [fetch_with_retry(session, self.api_url, self.headers, {'term': term, 'location': location, 'limit': self.BATCH_SIZE, 'offset': offset}) for offset in range(0, total, self.BATCH_SIZE)]\n",
    "                responses = await asyncio.gather(*tasks)\n",
    "                businesses = [item for response in responses for item in response.get('businesses', [])]\n",
    "            self._load_data(pd.DataFrame(businesses))\n",
    "        else:\n",
    "            logging.info(\"No results to fetch based on the provided search criteria.\")\n",
    "\n",
    "    def fetch_data(self, term, location, use_async=False):\n",
    "        logging.info(f\"Fetching data for {term} in {location} from {self.api_url}...\")\n",
    "        if use_async:\n",
    "            total = asyncio.run(self._fetch_total_results_async(term, location))\n",
    "            if total > 0:\n",
    "                asyncio.run(self._fetch_data_async(term, location, total))\n",
    "            else:\n",
    "                logging.info(\"No data found for the given search term and location.\")\n",
    "        else:\n",
    "            total = self._fetch_total_results_sync(term, location)\n",
    "            if total > 0:\n",
    "                businesses = self._fetch_data_sync(term, location, total)\n",
    "                self._load_data(pd.DataFrame(businesses))\n",
    "                logging.info(f\"Fetched and loaded {len(businesses)} records synchronously.\")\n",
    "            else:\n",
    "                logging.info(\"No data found for the given search term and location.\")\n",
    "\n",
    "    def _load_data(self, new_data):\n",
    "        if not new_data.empty:\n",
    "            # Reset DataFrame with new data while preserving metadata\n",
    "            self.__dict__.update(pd.DataFrame(new_data).__dict__)\n",
    "        else:\n",
    "            logging.info(\"No new data to load.\")\n",
    "\n",
    "# Example usage:\n",
    "# yelp_df = YelpDataFrame(yelp_key='YourYelpAPIKey')\n",
    "# yelp_df.fetch_data(term='hotels', location='Lisbon, Portugal', use_async=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T22:34:47.937064Z",
     "iopub.status.busy": "2024-03-14T22:34:47.937064Z",
     "iopub.status.idle": "2024-03-14T22:34:48.702792Z",
     "shell.execute_reply": "2024-03-14T22:34:48.702792Z",
     "shell.execute_reply.started": "2024-03-14T22:34:47.937064Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 18:34:47,939 - INFO - Fetching data for hotels in Lisbon, Portugal from https://api.yelp.com/v3/businesses/search...\n"
     ]
    }
   ],
   "source": [
    "yelp_df = YelpDataFrame(yelp_key=yelp_key)\n",
    "yelp_df.fetch_data(term='hotels', location='Lisbon, Portugal', use_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T22:34:48.702792Z",
     "iopub.status.busy": "2024-03-14T22:34:48.702792Z",
     "iopub.status.idle": "2024-03-14T22:34:48.791081Z",
     "shell.execute_reply": "2024-03-14T22:34:48.791081Z",
     "shell.execute_reply.started": "2024-03-14T22:34:48.702792Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\IPython\\core\\formatters.py:708\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    701\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[0;32m    702\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[0;32m    704\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[0;32m    705\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[0;32m    706\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[0;32m    707\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[1;32m--> 708\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\IPython\\lib\\pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    407\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[0;32m    408\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[0;32m    409\u001b[0m                         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m--> 410\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\IPython\\lib\\pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[1;34m(obj, p, cycle)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\pandas\\core\\frame.py:1133\u001b[0m, in \u001b[0;36mDataFrame.__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m   1132\u001b[0m repr_params \u001b[38;5;241m=\u001b[39m fmt\u001b[38;5;241m.\u001b[39mget_dataframe_repr_params()\n\u001b[1;32m-> 1133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_string(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrepr_params)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\pandas\\core\\frame.py:1310\u001b[0m, in \u001b[0;36mDataFrame.to_string\u001b[1;34m(self, buf, columns, col_space, header, index, na_rep, formatters, float_format, sparsify, index_names, justify, max_rows, max_cols, show_dimensions, decimal, line_width, min_rows, max_colwidth, encoding)\u001b[0m\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay.max_colwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_colwidth):\n\u001b[0;32m   1292\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m fmt\u001b[38;5;241m.\u001b[39mDataFrameFormatter(\n\u001b[0;32m   1293\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1294\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1308\u001b[0m         decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   1309\u001b[0m     )\n\u001b[1;32m-> 1310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfmt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_string\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mline_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mline_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\pandas\\io\\formats\\format.py:1100\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_string\u001b[1;34m(self, buf, encoding, line_width)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstring\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringFormatter\n\u001b[0;32m   1099\u001b[0m string_formatter \u001b[38;5;241m=\u001b[39m StringFormatter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt, line_width\u001b[38;5;241m=\u001b[39mline_width)\n\u001b[1;32m-> 1100\u001b[0m string \u001b[38;5;241m=\u001b[39m \u001b[43mstring_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m save_to_buffer(string, buf\u001b[38;5;241m=\u001b[39mbuf, encoding\u001b[38;5;241m=\u001b[39mencoding)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\pandas\\io\\formats\\string.py:29\u001b[0m, in \u001b[0;36mStringFormatter.to_string\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_string\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m---> 29\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_string_representation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt\u001b[38;5;241m.\u001b[39mshould_show_dimensions:\n\u001b[0;32m     31\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([text, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt\u001b[38;5;241m.\u001b[39mdimensions_info])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\pandas\\io\\formats\\string.py:44\u001b[0m, in \u001b[0;36mStringFormatter._get_string_representation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_empty_info_line\n\u001b[1;32m---> 44\u001b[0m strcols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_strcols\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_width \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# no need to wrap around just print the whole frame\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madj\u001b[38;5;241m.\u001b[39madjoin(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39mstrcols)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\pandas\\io\\formats\\string.py:35\u001b[0m, in \u001b[0;36mStringFormatter._get_strcols\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_strcols\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[1;32m---> 35\u001b[0m     strcols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strcols\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt\u001b[38;5;241m.\u001b[39mis_truncated:\n\u001b[0;32m     37\u001b[0m         strcols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_dot_separators(strcols)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\pandas\\io\\formats\\format.py:618\u001b[0m, in \u001b[0;36mDataFrameFormatter.get_strcols\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    615\u001b[0m strcols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_strcols_without_index()\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex:\n\u001b[1;32m--> 618\u001b[0m     str_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_formatted_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtr_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    619\u001b[0m     strcols\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, str_index)\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m strcols\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\pandas\\io\\formats\\format.py:983\u001b[0m, in \u001b[0;36mDataFrameFormatter._get_formatted_index\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    972\u001b[0m     fmt_index \u001b[38;5;241m=\u001b[39m [index\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_row_idx_names, formatter\u001b[38;5;241m=\u001b[39mfmt)]\n\u001b[0;32m    974\u001b[0m fmt_index \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    975\u001b[0m     \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m    976\u001b[0m         _make_fixed_width(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m fmt_index\n\u001b[0;32m    981\u001b[0m ]\n\u001b[1;32m--> 983\u001b[0m adjoined \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfmt_index\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# empty space for columns\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_col_idx_names:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\pandas\\io\\formats\\format.py:435\u001b[0m, in \u001b[0;36mTextAdjustment.adjoin\u001b[1;34m(self, space, *lists, **kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madjoin\u001b[39m(\u001b[38;5;28mself\u001b[39m, space: \u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m*\u001b[39mlists, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m--> 435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m printing\u001b[38;5;241m.\u001b[39madjoin(\n\u001b[0;32m    436\u001b[0m         space, \u001b[38;5;241m*\u001b[39mlists, strlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlen, justfunc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjustify, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    437\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\pandas\\io\\formats\\printing.py:50\u001b[0m, in \u001b[0;36madjoin\u001b[1;34m(space, *lists, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mmap\u001b[39m(strlen, x)) \u001b[38;5;241m+\u001b[39m space \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m lists[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# not the last one\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m lengths\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlists\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     51\u001b[0m maxLen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlen\u001b[39m, lists))\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, lst \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lists):\n",
      "\u001b[1;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>268 rows × 16 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T22:33:05.932423Z",
     "iopub.status.busy": "2024-03-14T22:33:05.932423Z",
     "iopub.status.idle": "2024-03-14T22:33:05.948006Z",
     "shell.execute_reply": "2024-03-14T22:33:05.948006Z",
     "shell.execute_reply.started": "2024-03-14T22:33:05.932423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty YelpDataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(yelp_df.head())  # Display the first few rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ƒ: yelp_request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:58:43.341776Z",
     "start_time": "2021-05-10T15:58:43.321768Z"
    }
   },
   "source": [
    " - Params: search term (eg. \"wineries); location; yelp_key variable (from Imports); and changing setting to print details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:23.110043Z",
     "start_time": "2021-05-13T18:49:23.097040Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# def yelp_request(term, location, yelp_key, verbose=True):\n",
    "#     '''Adapted from Yelp API Lab: https://github.com/BenJMcCarty/dsc-yelp-api-lab/tree/solution'''\n",
    "    \n",
    "#     url = 'https://api.yelp.com/v3/businesses/search'\n",
    "\n",
    "#     headers = {\n",
    "#             'Authorization': 'Bearer {}'.format(yelp_key),\n",
    "#         }\n",
    "\n",
    "#     url_params = {\n",
    "#                     'term': term.replace(' ', '+'),\n",
    "#                     'location': location.replace(' ', '+'),\n",
    "#                     'limit': 50\n",
    "#                 }\n",
    "#     response = requests.get(url, headers=headers, params=url_params)\n",
    "    \n",
    "#     if verbose == True:\n",
    "#         print(response)\n",
    "#         print(type(response.text))\n",
    "#         print(response.text[:1000])\n",
    "        \n",
    "#     return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adapted from code generated by ChatGPT - revised original function.\n",
    "\n",
    "def yelp_request(term: str, location: str, yelp_key: str, verbose: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Make a request to the Yelp API to search for businesses.\n",
    "\n",
    "    Parameters:\n",
    "    - term (str): The search term to query.\n",
    "    - location (str): The location to search within.\n",
    "    - yelp_key (str): Your Yelp API key.\n",
    "    - verbose (bool): Whether to print verbose output. Default is True.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing the JSON response from the Yelp API.\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://api.yelp.com/v3/businesses/search'\n",
    "\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {yelp_key}',\n",
    "    }\n",
    "\n",
    "    url_params = {\n",
    "        'term': term.replace(' ', '+'),\n",
    "        'location': location.replace(' ', '+'),\n",
    "        'limit': 50\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=url_params)\n",
    "        response.raise_for_status()  # Raise an exception for 4XX or 5XX status codes\n",
    "        if verbose:\n",
    "            print(response)\n",
    "            print(type(response.text))\n",
    "            print(response.text[:1000])\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T19:44:09.881783Z",
     "start_time": "2021-05-10T19:44:09.868735Z"
    }
   },
   "source": [
    "### Sending the request\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Un-comment the next line to run the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:23.126082Z",
     "start_time": "2021-05-13T18:49:23.113041Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# response = yelp_request('winery','Southern California', yelp_key)\n",
    "# response.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying and Exploring Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:23.142072Z",
     "start_time": "2021-05-13T18:49:23.129037Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Identify keys\n",
    "\n",
    "# print(response.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the \"Businesses\" Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:23.158083Z",
     "start_time": "2021-05-13T18:49:23.145043Z"
    }
   },
   "outputs": [],
   "source": [
    "# response['businesses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:23.174067Z",
     "start_time": "2021-05-13T18:49:23.160040Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Show first item w/in list of businesses\n",
    "\n",
    "# response['businesses'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:23.190067Z",
     "start_time": "2021-05-13T18:49:23.176038Z"
    }
   },
   "outputs": [],
   "source": [
    "# response['businesses'][0]['categories'][0]['alias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:23.206036Z",
     "start_time": "2021-05-13T18:49:23.195038Z"
    }
   },
   "outputs": [],
   "source": [
    "# response['businesses'][0]['categories'][0]['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the \"Total\" Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:23.222069Z",
     "start_time": "2021-05-13T18:49:23.209038Z"
    }
   },
   "outputs": [],
   "source": [
    "# response['total']\n",
    "\n",
    "# # How many businesses are there in total for my request?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the \"Region\" Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:23.238040Z",
     "start_time": "2021-05-13T18:49:23.226041Z"
    }
   },
   "outputs": [],
   "source": [
    "# response['region']\n",
    "\n",
    "# # From which geographical area will my results come?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T21:41:29.180856Z",
     "start_time": "2021-05-10T21:41:29.173856Z"
    }
   },
   "source": [
    "### ƒ: parse_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:23.253038Z",
     "start_time": "2021-05-13T18:49:23.241039Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# def parse_data(list_of_data):\n",
    "#     '''Adapted from Tyrell's code'''  \n",
    "\n",
    "#     # Create empty list to store results\n",
    "    \n",
    "#     parsed_data = []\n",
    "    \n",
    "#     # Loop through each business in the list of businesses\n",
    "#     # Add specific k:v pairs to a dictionary\n",
    "    \n",
    "#     for business in list_of_data:\n",
    "#         if 'price' not in business:\n",
    "#             business['price'] = np.nan\n",
    "            \n",
    "#             # Verify that the \"price\" key is in the selected business dict\n",
    "            \n",
    "#         details = {'name': business['name'],\n",
    "#                      'location': ' '.join(business['location']['display_address']),\n",
    "#                      'Business ID': business['id'],\n",
    "#                      'alias': business['categories'][0]['alias'],\n",
    "#                      'title': business['categories'][0]['title'],\n",
    "#                      'rating': business['rating'],\n",
    "#                      'review_count': business['review_count'],\n",
    "#                      'price': business['price'],\n",
    "#                      'latitude': business['coordinates']['latitude'],\n",
    "#                      'longitude': business['coordinates']['longitude']\n",
    "#                     }\n",
    "#         # Add the new dictionary to the previous list\n",
    "        \n",
    "#         parsed_data.append(details)\n",
    "    \n",
    "#     # Create a DataFrame from the resulting list\n",
    "    \n",
    "#     df_parsed_data = pd.DataFrame(parsed_data)\n",
    "\n",
    "    \n",
    "#     return df_parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adapted from code generated by ChatGPT - revised original function.\n",
    "\n",
    "def parse_data(list_of_data: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse a list of business data into a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - list_of_data (list): A list containing dictionaries of business data.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing parsed business data.\n",
    "    \"\"\"\n",
    "\n",
    "    parsed_data = []\n",
    "\n",
    "    for business in list_of_data:\n",
    "        # Handle missing or nested keys gracefully\n",
    "        price = business.get('price', np.nan)\n",
    "\n",
    "        # Avoid hardcoding category index\n",
    "        if 'categories' in business and business['categories']:\n",
    "            category = business['categories'][0]\n",
    "            alias = category.get('alias', '')\n",
    "            title = category.get('title', '')\n",
    "        else:\n",
    "            alias = ''\n",
    "            title = ''\n",
    "\n",
    "        details = {\n",
    "            'name': business.get('name', ''),\n",
    "            'location': ' '.join(business['location']['display_address']) if 'location' in business else '',\n",
    "            'Business ID': business.get('id', ''),\n",
    "            'alias': alias,\n",
    "            'title': title,\n",
    "            'rating': business.get('rating', np.nan),\n",
    "            'review_count': business.get('review_count', np.nan),\n",
    "            'price': price,\n",
    "            'latitude': business['coordinates']['latitude'] if 'coordinates' in business else np.nan,\n",
    "            'longitude': business['coordinates']['longitude'] if 'coordinates' in business else np.nan\n",
    "        }\n",
    "        parsed_data.append(details)\n",
    "\n",
    "    df_parsed_data = pd.DataFrame(parsed_data)\n",
    "\n",
    "    return df_parsed_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T22:37:53.049725Z",
     "start_time": "2021-05-10T22:37:53.042727Z"
    }
   },
   "source": [
    "## Updating Requests for Pagination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ƒ: yelp_request_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:23.268037Z",
     "start_time": "2021-05-13T18:49:23.257037Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# def yelp_request_offset(term, location, yelp_key, offset=0, verbose=False):\n",
    "#     '''Adapted from Yelp API Lab: https://github.com/BenJMcCarty/dsc-yelp-api-lab/tree/solution'''\n",
    "    \n",
    "#     url = 'https://api.yelp.com/v3/businesses/search'\n",
    "\n",
    "#     headers = {\n",
    "#             'Authorization': 'Bearer {}'.format(yelp_key),\n",
    "#         }\n",
    "\n",
    "#     url_params = {\n",
    "#                     'term': term.replace(' ', '+'),\n",
    "#                     'location': location.replace(' ', '+'),\n",
    "#                     'limit': 50,\n",
    "#                     'offset': offset\n",
    "#                         }\n",
    "    \n",
    "#     response = requests.get(url, headers=headers, params=url_params)\n",
    "    \n",
    "#     if verbose == True:\n",
    "#         print(response)\n",
    "#         print(type(response.text))\n",
    "#         print(response.text[:1000])\n",
    "        \n",
    "#     return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adapted from code generated by ChatGPT - revised original function.\n",
    "\n",
    "def yelp_request_offset(term: str, location: str, yelp_key: str, offset: int = 0, verbose: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Make a request to the Yelp API to search for businesses with an offset.\n",
    "\n",
    "    Parameters:\n",
    "    - term (str): The search term to query.\n",
    "    - location (str): The location to search within.\n",
    "    - yelp_key (str): Your Yelp API key.\n",
    "    - offset (int): The offset for paginating results. Default is 0.\n",
    "    - verbose (bool): Whether to print verbose output. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing the JSON response from the Yelp API.\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://api.yelp.com/v3/businesses/search'\n",
    "\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {yelp_key}',\n",
    "    }\n",
    "\n",
    "    url_params = {\n",
    "        'term': term.replace(' ', '+'),\n",
    "        'location': location.replace(' ', '+'),\n",
    "        'limit': 50,\n",
    "        'offset': offset\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=url_params)\n",
    "        response.raise_for_status()  # Raise an exception for 4XX or 5XX status codes\n",
    "        if verbose:\n",
    "            print(response)\n",
    "            print(type(response.text))\n",
    "            print(response.text[:1000])\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T21:52:21.177479Z",
     "start_time": "2021-05-10T21:52:21.167481Z"
    }
   },
   "source": [
    "#### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:23.283047Z",
     "start_time": "2021-05-13T18:49:23.271041Z"
    }
   },
   "outputs": [],
   "source": [
    "# test1 = yelp_request_offset('winery', 'San Diego', yelp_key)\n",
    "# test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:23.298037Z",
     "start_time": "2021-05-13T18:49:23.285039Z"
    }
   },
   "outputs": [],
   "source": [
    "# test1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:23.313037Z",
     "start_time": "2021-05-13T18:49:23.300039Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test1['total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:23.329038Z",
     "start_time": "2021-05-13T18:49:23.315039Z"
    }
   },
   "outputs": [],
   "source": [
    "# test1['businesses'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ƒ: GET BUSINESSES (ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:23.345071Z",
     "start_time": "2021-05-13T18:49:23.331039Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# def get_full_data(term, location, yelp_key, file_name = 'data/wineries_raw.csv'):\n",
    "#     '''Requests all results from Yelp API; saves as a .csv; and returns a DataFrame.'''\n",
    "\n",
    "#     # Create a .csv to store results\n",
    "#     blank_df = pd.DataFrame()\n",
    "#     blank_df.to_csv(file_name)\n",
    "    \n",
    "#     # Process first request to Yelp API and calculate number of pages \n",
    "#     results = yelp_request_offset(term, location, yelp_key, offset=0, \n",
    "#                                   verbose=False)\n",
    "#     num_pages = results['total']//50+1\n",
    "    \n",
    "#     # Print out confirmation feedback\n",
    "#     print(f'For {term} and {location}: ')\n",
    "#     print(f\"    Total number of results: {results['total']}.\")\n",
    "#     print(f'    Total number of pages: {num_pages}.')\n",
    "    \n",
    "#     # Create offset for additional results\n",
    "#     offset = 0\n",
    "\n",
    "#     # Retrieves remaining pages\n",
    "#     for num in range(num_pages-1):\n",
    "#         try:\n",
    "#             # Process API request\n",
    "#             results = yelp_request_offset(term, location, yelp_key,\n",
    "#                                           offset=offset, verbose=False)\n",
    "            \n",
    "#             # From results, take values from \"Businesses\" key and save\n",
    "#             parsed_results = parse_data(results['businesses'])\n",
    "          \n",
    "#             # Save resulting DF to .csv from top\n",
    "#             parsed_results.to_csv(file_name, mode='a', index = False)\n",
    "            \n",
    "#             # Increase offset to move to next \"page\" of data\n",
    "#             offset += 50\n",
    "            \n",
    "#         except:\n",
    "#             # If error, print where the error happens\n",
    "#             print(f'Error on page {num}.')\n",
    "#             # Then save the results so far to the .csv\n",
    "#             parsed_results.to_csv(file_name, mode='a', index = False)\n",
    "\n",
    "\n",
    "#     return parsed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adapted from code generated by ChatGPT - revised original function.\n",
    "\n",
    "def get_full_data(term: str, location: str, yelp_key: str, file_name: str = 'data/wineries_raw.csv') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Requests all results from Yelp API, saves as a .csv, and returns a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - term (str): The search term to query.\n",
    "    - location (str): The location to search within.\n",
    "    - yelp_key (str): Your Yelp API key.\n",
    "    - file_name (str): The filename for the CSV file to save the data. Default is 'data/wineries_raw.csv'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing parsed business data.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Check if the file already exists\n",
    "        with open(file_name, 'x'):\n",
    "            pass\n",
    "    except FileExistsError:\n",
    "        print(f\"File '{file_name}' already exists.\")\n",
    "\n",
    "    # Process first request to Yelp API and calculate number of pages \n",
    "    results = yelp_request_offset(term, location, yelp_key, offset=0, verbose=False)\n",
    "    num_pages = (results.get('total', 0) // 50) + 1\n",
    "    \n",
    "    # Print out confirmation feedback\n",
    "    print(f'For {term} and {location}: ')\n",
    "    print(f\"    Total number of results: {results.get('total', 0)}.\")\n",
    "    print(f'    Total number of pages: {num_pages}.')\n",
    "\n",
    "    # Create offset for additional results\n",
    "    offset = 0\n",
    "\n",
    "    # Retrieves remaining pages\n",
    "    for num in range(num_pages):\n",
    "        try:\n",
    "            # Process API request\n",
    "            results = yelp_request_offset(term, location, yelp_key, offset=offset, verbose=False)\n",
    "            \n",
    "            # From results, take values from \"businesses\" key and save\n",
    "            parsed_results = parse_data(results.get('businesses', []))\n",
    "          \n",
    "            # Save resulting DF to .csv from top\n",
    "            with open(file_name, 'a') as file:\n",
    "                parsed_results.to_csv(file, mode='a', index=False, header=(num == 0))\n",
    "            \n",
    "            # Increase offset to move to next \"page\" of data\n",
    "            offset += 50\n",
    "            \n",
    "        except (KeyError, IndexError) as e:\n",
    "            # If error, print where the error happens\n",
    "            print(f'Error on page {num}: {e}')\n",
    "            # Then break the loop to avoid infinite retries\n",
    "            break\n",
    "\n",
    "    return parsed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:28.285042Z",
     "start_time": "2021-05-13T18:49:23.347038Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = get_full_data('hotel', 'Lisbon', yelp_key, file_name = './data/Hotel_Reviews_Lisbon.csv')\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ƒ: clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying, Filtering for, and Saving Top 2 Aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:28.300042Z",
     "start_time": "2021-05-13T18:49:28.288044Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def sort_by_aliases(raw_data = 'data/wineries_raw.csv'):\n",
    "\n",
    "#     # Read in businesses\n",
    "#     df1 = pd.read_csv(raw_data, header = 0)\n",
    "\n",
    "#     # Create new DF filtering alias and title columns\n",
    "#     df1_alias = df1.loc[:,['alias', 'title']]\n",
    "\n",
    "#     # Identify top 2 aliases \n",
    "#     df1_alias_count = df1_alias.groupby('alias').count().sort_values(['title'],\\\n",
    "#                                                             ascending=False)[:2]\n",
    "\n",
    "#     # Note: initially tried top 3, but it returned distributors, not wineries\n",
    "\n",
    "#     df1_alias_count.reset_index(inplace=True)\n",
    "    \n",
    "#     print(\"Top two aliases: \")\n",
    "#     print(df1_alias_count)\n",
    "\n",
    "#     # display them as a list\n",
    "#     aliases_top_2 = df1_alias_count['alias'].tolist()\n",
    "\n",
    "#     # Selecting rows based on condition and saving\n",
    "\n",
    "#     df2 = df1[df1['alias'].isin(aliases_top_2)]\n",
    "\n",
    "#     df2.to_csv('data/wineries_filtered_alias.csv', index = False)\n",
    "#     print(\"Saved to 'data/wineries_filtered_alias.csv'\")\n",
    "    \n",
    "#     return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:28.315044Z",
     "start_time": "2021-05-13T18:49:28.303041Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_data(existing_dataframe, raw_data_path, cleaned_data_path):\n",
    "    '''- Requires data from either an existing dataframe or an existing .csv file\n",
    "    - Takes raw business data from the Yelp API and filters for the top two\n",
    "    aliases (focusing on \"wineries\" and \"winetastingrooms\").\n",
    "    '''\n",
    "\n",
    "    # Read in businesses\n",
    "    df1 = pd.read_csv(raw_data_path, header = 1)\n",
    "\n",
    "    alias_index = df1['alias'].value_counts()[:2].index\n",
    "    print(alias_index)\n",
    "    \n",
    "    # Filtering rows based on condition\n",
    "\n",
    "    df2 = df1[df1['alias'].isin(alias_index)]\n",
    "    \n",
    "    # Resetting index\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Save results\n",
    "    df2.to_csv(cleaned_data_path,index = False)\n",
    "       \n",
    "    print(f\"Saved to {cleaned_data_path}\")\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:28.379040Z",
     "start_time": "2021-05-13T18:49:28.318043Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_data_path='./data/Hotel_Reviews_Lisbon.csv'\n",
    "cleaned_data_path='./data/Hotel_Reviews_Lisbon_Cleaned.csv'\n",
    "\n",
    "df_filtered = clean_data(existing_dataframe = None,\n",
    "                         raw_data_path = raw_data_path,\n",
    "                         cleaned_data_path = cleaned_data_path)\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Classy - Creating Class to Handle Full Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Code initially generated by ChatGPT and adapted further.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpReviewManager:\n",
    "    def __init__(self, yelp_key):\n",
    "        \"\"\"Initialize with Yelp API key.\"\"\"\n",
    "        self._yelp_key = yelp_key\n",
    "        self._base_url = 'https://api.yelp.com/v3/businesses/search'\n",
    "        self.raw_data = None\n",
    "        self.transformed_data = None\n",
    "\n",
    "    def _make_api_request(self, term, location, offset=0):\n",
    "        \"\"\"Make a private Yelp API request.\"\"\"\n",
    "        headers = {'Authorization': f'Bearer {self._yelp_key}'}\n",
    "        params = {'term': term, 'location': location, 'limit': 50, 'offset': offset}\n",
    "        response = requests.get(self._base_url, headers=headers, params=params)\n",
    "        return response.json()\n",
    "\n",
    "    def fetch_and_save_raw_data(self, term, location, file_name, verbose=False):\n",
    "        \"\"\"Fetch raw data based on term and location, and save it.\"\"\"\n",
    "        businesses = []\n",
    "        offset = 0\n",
    "        while True:\n",
    "            response = self._make_api_request(term, location, offset)\n",
    "            batch = response.get('businesses', [])\n",
    "            if not batch or 'total' in response and len(businesses) >= response['total']:\n",
    "                break\n",
    "            businesses.extend(batch)\n",
    "            offset += 50\n",
    "            if verbose:\n",
    "                print(f\"Fetched {len(batch)} businesses, total: {len(businesses)}\")\n",
    "        self.raw_data = pd.DataFrame(businesses)\n",
    "        self.raw_data.to_csv(file_name, index=False)\n",
    "        print(f\"Raw data saved to {file_name}.\")\n",
    "\n",
    "    def transform_data(self, top_n=2):\n",
    "        \"\"\"Transform the raw data by filtering based on top n aliases.\"\"\"\n",
    "        if self.raw_data is None:\n",
    "            raise ValueError(\"Raw data is not fetched yet.\")\n",
    "        value_counts = self.raw_data['categories'].apply(lambda x: x[0]['alias'] if x else None).value_counts()\n",
    "        top_aliases = value_counts.nlargest(top_n).index.tolist()\n",
    "        self.transformed_data = self.raw_data[self.raw_data['categories'].apply(lambda x: x[0]['alias'] if x else None).isin(top_aliases)]\n",
    "\n",
    "    def save_transformed_data(self, file_name):\n",
    "        \"\"\"Save the transformed data to a CSV file.\"\"\"\n",
    "        if self.transformed_data is None:\n",
    "            raise ValueError(\"Data has not been transformed yet.\")\n",
    "        self.transformed_data.to_csv(file_name, index=False)\n",
    "        print(f\"Transformed data saved to {file_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the YelpReviewManager with your Yelp API key\n",
    "yelp_review_manager = YelpReviewManager(yelp_key=yelp_key)\n",
    "\n",
    "# Fetch and save raw data for hotels in Lisbon, Portugal\n",
    "yelp_review_manager.fetch_and_save_raw_data(term='hotels', location='Lisbon, Portugal', file_name='lisbon_hotels_raw.csv', verbose=True)\n",
    "\n",
    "# Perform transformation on the fetched data\n",
    "yelp_review_manager.transform_data(top_n=2)\n",
    "\n",
    "# Save the transformed data\n",
    "yelp_review_manager.save_transformed_data(file_name='lisbon_hotels_transformed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T17:51:05.843625Z",
     "start_time": "2021-05-13T17:51:05.832627Z"
    }
   },
   "source": [
    "# ƒ: convert_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:28.394040Z",
     "start_time": "2021-05-13T18:49:28.382043Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_price(dataframe, filepath):\n",
    "    ''' - Requires a dataframe with the 'price' column elements being NaN, $, $$, or $$$.\n",
    "    - Takes a pre-existing dataframe and adds a column to store the conversion from $ to an integer.'''\n",
    "    \n",
    "    # Converting $s to integers, then saving to new column.\n",
    "    dataframe['price_converted'] = dataframe.loc[:,'price'].map({np.nan:0, '$':1, '$$':2, '$$$':3})\n",
    "    \n",
    "    \n",
    "    # Saves results to new file\n",
    "    dataframe.to_csv(filepath,index = False)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:50:53.757364Z",
     "start_time": "2021-05-13T18:50:53.710366Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convert_price(dataframe = df_filtered, filepath = './data/Hotel_Reviews_Lisbon_Cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 -  Create ETL pipeline for the restaurant review data from the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've done this for the Businesses, now you need to do this for reviews. You will follow the same process, but your functions will be specific to reviews. Above you have a model of the functions you will need to write, and how to pull them together in one script. For this part, you have the process below "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Business IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to pull the reveiws, you will need the business ids. So your first step will be to get all of the business ids from your businesses csv. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open file and slice ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open data/wineries.csv\n",
    "2. Slice out the 'name' and 'id' columns for each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:28.441042Z",
     "start_time": "2021-05-13T18:49:28.415042Z"
    }
   },
   "outputs": [],
   "source": [
    "df_saved = pd.read_csv(\"./data/Hotel_Reviews_Lisbon_Cleaned.csv\")\n",
    "df_saved.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Slice out IDs, then save them to a list\n",
    "\n",
    "df_saved_id = df_saved['Business ID'].to_list()\n",
    "       \n",
    "len(df_saved_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requesting Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function that takes a business id and makes a call to the API for reviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ƒ: get_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:28.457041Z",
     "start_time": "2021-05-13T18:49:28.443041Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_reviews(business_ID, yelp_key, verbose=False):\n",
    "    '''Adapted from Yelp API Lab: https://github.com/BenJMcCarty/dsc-yelp-api-lab/tree/solution'''\n",
    "    \n",
    "    url = 'https://api.yelp.com/v3/businesses/'+ business_ID + '/reviews'\n",
    "\n",
    "    headers = {\n",
    "            'Authorization': 'Bearer {}'.format(yelp_key),\n",
    "        }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if verbose == True:\n",
    "        print(response)\n",
    "        print(type(response.text))\n",
    "        print(response.text[:1000])\n",
    "\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ƒ: Parse Reviews for final GET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:28.473044Z",
     "start_time": "2021-05-13T18:49:28.459041Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def parse_reviews(review):\n",
    "    '''Adapted from Tyrell's code'''  \n",
    "\n",
    "    \n",
    "    # Loop through each review in the list of reviews\n",
    "    # Add specific k:v pairs to a dictionary      \n",
    "    details = {\n",
    "        'Reviewer Name': review['user']['name'],\n",
    "        'Review Rating': review['rating'],\n",
    "        'Review Text': review['text'],\n",
    "        'Time Created': review['time_created']\n",
    "        }\n",
    "\n",
    "\n",
    "    # Create a DataFrame from the resulting dictionary\n",
    "    \n",
    "    df_parsed_reviews = pd.DataFrame.from_dict([details])\n",
    "   \n",
    "    return df_parsed_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T18:10:38.397744Z",
     "start_time": "2021-05-12T18:10:38.381718Z"
    }
   },
   "source": [
    "# ƒ: GET REVIEWS (ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:28.489040Z",
     "start_time": "2021-05-13T18:49:28.476045Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_reviews(list_of_biz_ids, yelp_key, file_name = 'data/hotel_reviews_raw.csv'):\n",
    "    '''Requests all review results for given business IDs from Yelp API; \\\n",
    "    saves as a .csv; and returns a DataFrame.'''\n",
    "    \n",
    "    # Create a starter empty DataFrame and save to .csv to store data.    \n",
    "    blank_df = pd.DataFrame(columns= ['Reviewer Name', 'Review Rating', \n",
    "                                      'Review Text', 'Time Created', \n",
    "                                      'Business ID'])\n",
    "    blank_df.to_csv(file_name, index = False)\n",
    "        \n",
    "    for i in list_of_biz_ids:\n",
    "        try:\n",
    "            \n",
    "            # Process API request for 3 reviews per business:\n",
    "            raw_reviews = get_reviews(i, yelp_key)\n",
    "\n",
    "            for review in raw_reviews['reviews']:\n",
    "                \n",
    "\n",
    "                # From results, take values from \"Businesses\" key and save\n",
    "                parsed_reviews = parse_reviews(review) \n",
    "\n",
    "                parsed_reviews['Business ID'] = i\n",
    "                \n",
    "                # H2: save results to df\n",
    "                parsed_reviews.to_csv(file_name, mode='a', index = False,\n",
    "                                      header = False)\n",
    "\n",
    "        except:\n",
    "            # If error, print where the error happens\n",
    "            print(f'Error on page {num}.')\n",
    "            # Then save the results so far to the .csv\n",
    "            parsed_reviews.to_csv(file_name, mode='a', index = False, \n",
    "                                  header = False)\n",
    "\n",
    "    try:\n",
    "        reviews1 = pd.read_csv(file_name)\n",
    "        return reviews1\n",
    "    except:\n",
    "        return parsed_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test GET REVIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.159658Z",
     "start_time": "2021-05-13T18:49:28.492042Z"
    }
   },
   "outputs": [],
   "source": [
    "test_all_funct = get_all_reviews(df_saved_id, yelp_key)\n",
    "test_all_funct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.175691Z",
     "start_time": "2021-05-13T18:49:57.161660Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_details = pd.read_csv(\"data/wineries_price_converted.csv\")\n",
    "# df_reviews = pd.read_csv(\"data/reviews_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.191658Z",
     "start_time": "2021-05-13T18:49:57.177668Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_details.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.207666Z",
     "start_time": "2021-05-13T18:49:57.194661Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.223662Z",
     "start_time": "2021-05-13T18:49:57.209659Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_merged = df_details.merge(df_reviews, how='outer', on='Business ID')\n",
    "# df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.239695Z",
     "start_time": "2021-05-13T18:49:57.225658Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 -  Using python and pandas, write code to answer the questions below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reviews**\n",
    "\n",
    "Which are the 5 most reviewed businesses in your dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.255659Z",
     "start_time": "2021-05-13T18:49:57.241662Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_saved.sort_values('review_count', ascending=False).head()[['name','review_count']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the highest rating received in your data set and how many businesses have that rating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.271697Z",
     "start_time": "2021-05-13T18:49:57.257658Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# max_rating = df_saved['rating'].max()\n",
    "\n",
    "# df_saved[df_saved['rating']== max_rating].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.287691Z",
     "start_time": "2021-05-13T18:49:57.274691Z"
    }
   },
   "outputs": [],
   "source": [
    "# sns.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.302689Z",
     "start_time": "2021-05-13T18:49:57.290674Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Price to Int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.318689Z",
     "start_time": "2021-05-13T18:49:57.304662Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_saved['price'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.333666Z",
     "start_time": "2021-05-13T18:49:57.321663Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_saved['price_converted'] = df_saved['price'].map({np.nan:0, '$':1, '$$':2, '$$$':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.364660Z",
     "start_time": "2021-05-13T18:49:57.342659Z"
    }
   },
   "outputs": [],
   "source": [
    "# sns.countplot(x= 'price_converted', data=df_saved);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.379658Z",
     "start_time": "2021-05-13T18:49:57.367659Z"
    }
   },
   "outputs": [],
   "source": [
    "# sns.countplot(x= 'rating',data=df_saved);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- `hue` parameter - seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What percentage of businesses have a rating greater than or  4.5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.395657Z",
     "start_time": "2021-05-13T18:49:57.381659Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_total_high = df_saved[df_saved['rating'] >= 4.5].shape[0]\n",
    "# df_total_high / df_saved.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What percentage of businesses have a rating less than 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.411658Z",
     "start_time": "2021-05-13T18:49:57.397660Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_total_low = df_saved[df_saved['rating'] < 3].shape[0]\n",
    "# df_total_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.426660Z",
     "start_time": "2021-05-13T18:49:57.414659Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_total_low / df_saved.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Pricing**\n",
    "\n",
    "What percentage of your businesses have a price label of one dollar sign? Two dollar signs? Three dollar signs? No dollar signs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.442660Z",
     "start_time": "2021-05-13T18:49:57.429660Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_total_zero = df_saved[df_saved['price_converted'] == 0].shape[0]\n",
    "# print(df_total_zero / df_saved.shape[0])\n",
    "# df_total_one = df_saved[df_saved['price_converted'] == 1].shape[0]\n",
    "# print(df_total_one / df_saved.shape[0])\n",
    "# df_total_two = df_saved[df_saved['price_converted'] == 2].shape[0]\n",
    "# print(df_total_two / df_saved.shape[0])\n",
    "# df_total_three = df_saved[df_saved['price_converted'] == 3].shape[0]\n",
    "# print(df_total_three / df_saved.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Returing Reviews**\n",
    "\n",
    "Return the text of the reviews for the most reviewed business. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.457670Z",
     "start_time": "2021-05-13T18:49:57.444660Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_saved.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.473657Z",
     "start_time": "2021-05-13T18:49:57.460661Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# max_reviewed = df_saved['review_count'].max()\n",
    "# most_reviewed_id = df_saved[df_saved['review_count'] == max_reviewed]['business id']\n",
    "# most_reviewed_id\n",
    "# # test_all_funct[test_all_funct['business id'] == 'DknnpiG1p4OoM1maFshzXA']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.488659Z",
     "start_time": "2021-05-13T18:49:57.476662Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_all_funct.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the highest rated business and return text of the most recent review. If multiple business have the same rating, select the business with the most reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.503662Z",
     "start_time": "2021-05-13T18:49:57.491662Z"
    }
   },
   "outputs": [],
   "source": [
    "# top_b = df_saved[df_saved['rating'] == max_rating]\n",
    "# max_counts = df_saved['review_count'].max()\n",
    "# top_num_reviews_top_b = top_b['review_count'].max()\n",
    "# top_b[top_b['review_count'] == top_num_reviews_top_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the lowest rated business and return text of the most recent review.  If multiple business have the same rating, select the business with the least reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.519662Z",
     "start_time": "2021-05-13T18:49:57.506661Z"
    }
   },
   "outputs": [],
   "source": [
    "# min_b = df_saved['rating'].min()\n",
    "# min_b = df_saved[df_saved['rating'] == min_b]\n",
    "# min_counts = df_saved['review_count'].min()\n",
    "# min_num_reviews_min_b = min_b['review_count'].min()\n",
    "# min_b[min_b['review_count'] == min_num_reviews_min_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pagination\n",
    "\n",
    "Returning to the Yelp API, the [documentation](https://www.yelp.com/developers/documentation/v3/business_search) also provides us details regarding the API limits. These often include details about the number of requests a user is allowed to make within a specified time limit and the maximum number of results to be returned. In this case, we are told that any request has a maximum of 50 results per request and defaults to 20. Furthermore, any search will be limited to a total of 1000 results. To retrieve all 1000 of these results, we would have to page through the results piece by piece, retriving 50 at a time. Processes such as these are often refered to as pagination.\n",
    "\n",
    "Now that you have an initial response, you can examine the contents of the json container. For example, you might start with ```response.json().keys()```. Here, you'll see a key for `'total'`, which tells you the full number of matching results given your query parameters. Write a loop (or ideally a function) which then makes successive API calls using the offset parameter to retrieve all of the results (or 5000 for a particularly large result set) for the original query. As you do this, be mindful of how you store the data. \n",
    "\n",
    "**Note: be mindful of the API rate limits. You can only make 5000 requests per day, and APIs can make requests too fast. Start prototyping small before running a loop that could be faulty. You can also use time.sleep(n) to add delays. For more details see https://www.yelp.com/developers/documentation/v3/rate_limiting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Below is sample code that you can use to help you deal with the pagination parameter and bring all of the functions together.***\n",
    "\n",
    "\n",
    "***Also, something might cause your code to break while it is running. You don't want to constantly repull the same data when this happens, so you should insert the data into the database as you call and parse it, not after you have all of the data***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T18:49:57.535659Z",
     "start_time": "2021-05-13T18:49:57.522660Z"
    }
   },
   "outputs": [],
   "source": [
    "# # create a variable  to keep track of which result you are in. \n",
    "# cur = 0\n",
    "\n",
    "# #set up a while loop to go through and grab the result \n",
    "# while cur < num and cur < 1000:\n",
    "#     #set the offset parameter to be where you currently are in the results \n",
    "#     url_params['offset'] = cur\n",
    "#     #make your API call with the new offset number\n",
    "#     results = yelp_call(url_params, api_key)\n",
    "    \n",
    "#     #after you get your results you can now use your function to parse those results\n",
    "#     parsed_results = parse_results(results)\n",
    "    \n",
    "#     # use your function to insert your parsed results into the db\n",
    "#     db_insert(parsed_results)\n",
    "#     #increment the counter by 50 to move on to the next results\n",
    "#     cur += 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "225px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": "32",
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
